# Complete Data Sources - Comprehensive Breakdown

## TL;DR: Data Usage Summary

| Component | Data Source | Files | Purpose | Stored In |
|-----------|-------------|-------|---------|-----------|
| **Training** | Medical docs | 5 files in `Data/` | Knowledge base | Pinecone |
| **Evaluation** | Q&A pairs | `qa_dataset.json` | Test performance | Local JSON |
| **MCP Local** | Datasets | JSON/CSV in `Data/` | Keyword search | Memory |
| **Web Search** | Real-time web | Exa AI API | Latest info | Online |
| **Chat History** | User messages | Session memory | Context | Memory |

---

## 1. TRAINING DATA (RAG Knowledge Base)

### Location: `Data/` directory

```
ğŸ“ Data/
â”œâ”€â”€ The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf
â”œâ”€â”€ medquad_dataset.json
â”œâ”€â”€ medical_conditions.json
â”œâ”€â”€ medical_diseases.csv
â””â”€â”€ mimic_iv_reference.json
```

### Processing Pipeline

```
Data/ files
    â†“
store_index.py loads via load_mixed_data()
    â†“
Text extraction:
- PDF â†’ text via PyPDF
- JSON â†’ parse items
- CSV â†’ parse rows
    â†“
text_split() â†’ chunks ~500-1000 tokens each
    â†“
download_hugging_face_embeddings() â†’ 384-dim vectors
    â†“
PineconeVectorStore.from_documents()
    â†“
Stored in Pinecone:
- Index: "medicalbot"
- Namespace: "default"
- Total: ~10K+ embeddings
```

### Each File's Role

#### 1. **The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf**
- **Source**: Medical encyclopedia (authoritative)
- **Content**: Comprehensive medical information
- **Format**: PDF (extracted via PyPDF)
- **Size**: Large (~1000+ pages)
- **Usage**: General medical knowledge, symptoms, diseases
- **Chunks Created**: ~2000+
- **Quality**: Very high (encyclopedic)

#### 2. **medquad_dataset.json**
- **Source**: MedQuAD (freely available medical Q&A)
- **Content**: Q&A pairs on medical topics
- **Format**: JSON with structure `{question, answer}`
- **Size**: ~16K Q&A pairs (limited to 100 used)
- **Usage**: Medical Q&A knowledge
- **Chunks Created**: ~1500+
- **Quality**: High (curated Q&A)

#### 3. **medical_conditions.json**
- **Source**: Ingested dataset (MIMIC-IV inspired)
- **Content**: 59 medical conditions with structured info
- **Format**: JSON array of condition objects
- **Fields**: name, description, symptoms, treatment, etc.
- **Usage**: Structured condition lookup
- **Chunks Created**: ~500+
- **Quality**: Medium (structured data)
- **Also used by**: MCP local search

#### 4. **medical_diseases.csv**
- **Source**: Ingested dataset
- **Content**: 6 diseases with basic information
- **Format**: CSV with columns
- **Usage**: Disease quick reference
- **Chunks Created**: ~100+
- **Quality**: Medium (minimal data)
- **Also used by**: MCP local search

#### 5. **mimic_iv_reference.json**
- **Source**: MIMIC-IV inspired reference
- **Content**: 5 clinical documents with ICU data
- **Format**: JSON with clinical scenarios
- **Usage**: Real-world medical scenarios, diagnoses
- **Chunks Created**: ~200+
- **Quality**: High (realistic)
- **Also used by**: MCP local search

### How Training Data is Searched

```
User Question: "What is diabetes?"
    â†“
Question converted to 384-dim embedding
    â†“
Pinecone searches for similar embeddings
    â†“
MMR (Max Marginal Relevance) retrieval:
- Retrieves k=8 chunks
- Fetches k=20 candidates
- Balances relevance + diversity (lambda=0.5)
    â†“
Top 8 chunks returned to LLM
    â†“
LLM generates answer
```

---

## 2. EVALUATION DATA (Test Set)

### Location: `docs/evaluation/`

```
ğŸ“ docs/evaluation/
â”œâ”€â”€ qa_dataset.json              â† Main evaluation Q&A
â”œâ”€â”€ qa_dataset_expanded.json     â† Extended version
â”œâ”€â”€ evaluate_with_visualization.py
â”œâ”€â”€ ragas_results.csv
â””â”€â”€ qa_dataset.json
```

### QA Dataset Structure

```json
[
  {
    "question": "What are the common symptoms of diabetes mellitus?",
    "ground_truth": "Common symptoms include frequent urination, excessive thirst..."
  },
  {
    "question": "How is hypertension typically treated?",
    "ground_truth": "Treatment includes lifestyle modifications and medications..."
  }
]
```

### Dataset Statistics

- **Total Q&A pairs**: ~50-100 (configurable)
- **Format**: JSON array
- **Independence**: Not from training data
- **Quality**: Medium-High (manually curated)
- **Coverage**: Diverse medical topics

### Evaluation Process

```
For each Q&A pair (50 iterations):

1. Load question
   
2. Query RAG system:
   a. Convert question to embedding
   b. Search Pinecone
   c. Retrieve 8 documents
   
3. Generate answer:
   a. Pass question + docs to LLM
   b. Get chatbot answer
   
4. Extract contexts:
   a. Get retrieved document chunks
   
5. Prepare evaluation data:
   {
     "question": "What are common symptoms?",
     "answer": "Chatbot's answer",
     "contexts": [...retrieved chunks...],
     "reference": "Ground truth answer"
   }
   
6. Repeat for all pairs
```

### RAGAS Metrics Computed

```
For each Q&A pair:

1. Faithfulness
   - Does answer match retrieved contexts?
   - Score: 0-1
   
2. Answer Relevancy
   - Is answer relevant to question?
   - Score: 0-1
   
3. Context Precision
   - Are retrieved documents relevant?
   - Score: 0-1
   
4. Context Recall
   - Do documents cover ground truth?
   - Score: 0-1
```

### Results Storage

```
ragas_results.csv:
question,answer,contexts,reference,faithfulness,answer_relevancy,...
"What is diabetes?","Diabetes is...",["chunk1","chunk2"],"Ground truth",0.85,0.92,...

ragas_summary.json:
{
  "faithfulness_mean": 0.82,
  "answer_relevancy_mean": 0.88,
  "context_precision_mean": 0.79,
  "context_recall_mean": 0.91
}
```

---

## 3. MCP LOCAL DATA (Ingested Datasets)

### What is MCP?
Model Context Protocol - local data ingestion and search

### Available Datasets

```
User can ingest any JSON/CSV file:

Examples:
- medical_conditions.json
- medical_diseases.csv
- mimic_iv_reference.json
- custom_dataset.json (user-provided)
```

### How MCP Search Works

```
User: "What are symptoms of asthma?"
    â†“
MCP keyword extraction: ["symptoms", "asthma"]
    â†“
Search ingested datasets:
- medical_conditions.json: Found "asthma" entry
- medical_diseases.csv: No match
- mimic_iv_reference.json: Found in ICU diagnoses
    â†“
Keyword relevance scoring (0-1)
    â†“
Top 10 results returned
    â†“
Show "ğŸ“Š MCP (Local Data)" in source attribution
```

### MCP Storage

```
In memory (MCPDatasetServer):
- Loaded documents list
- Ingested dataset metadata
- Fast keyword search
```

### Usage in Chat

```
Answers combine:
- RAG search (Pinecone semantic)
- MCP search (Local keyword)
- Exa web search (Real-time)
    â†“
All sources shown in source attribution box
```

---

## 4. WEB SEARCH DATA (Exa AI)

### What Data Comes from Web?

```
Real-time medical websites:
- Mayo Clinic
- WebMD
- MedlinePlus
- NIH.gov
- CDC.gov
- WHO.int
- Healthline.com
- PubMed
- NHS.uk
```

### Web Search Process

```
Question: "Latest COVID-19 variants"
    â†“
Medical query check: YES
    â†“
Exa AI search:
- Search query: "Latest COVID-19 variants medical health"
- Type: Neural search
- Domains: Medical sites only
- Exclude: Social media
    â†“
Real-time results from web
    â†“
Show "ğŸŒ Web Search" in source attribution
```

### Data Not Stored Locally

- Retrieved on-demand from Exa API
- Not cached (always fresh)
- Shows current information
- URLs included in response

---

## 5. CONVERSATION HISTORY (Session Data)

### Storage

```
In-memory conversation buffer:
ConversationBufferWindowMemory
- Max tokens: 1000
- Window: Last N messages
- Cleared on app restart
```

### Usage

```
Question 1: "What is diabetes?"
    â†“ (stored in memory)

Question 2: "What are treatments?"
    â†“ Context includes Q1 for multi-turn
    â†“ (stored in memory)

Question 3: "Anything else?"
    â†“ Can reference both Q1 and Q2
```

---

## Complete Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  USER QUERY                             â”‚
â”‚         "What is type 2 diabetes?"                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            â”‚            â”‚
        â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Trainingâ”‚  â”‚Local MCP â”‚ â”‚Exa Web  â”‚
    â”‚ Data    â”‚  â”‚Data      â”‚ â”‚Search   â”‚
    â”‚(Pinecone)  â”‚(Memory)  â”‚ â”‚(API)    â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚             â”‚           â”‚
        â”‚ RAG Search  â”‚ Keyword   â”‚ Real-time
        â”‚ Semantic    â”‚ Matching  â”‚ Web
        â”‚             â”‚           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Combine all   â”‚
              â”‚ 3 sources     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Send context  â”‚
              â”‚ to GPT-4o-minâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Generate      â”‚
              â”‚ Answer        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Return response:     â”‚
              â”‚ - Answer text        â”‚
              â”‚ - Sources used       â”‚
              â”‚ - Source breakdown   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  User sees chat  â”‚
              â”‚  with attributionâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Lifecycle

### Training Data
```
Data/ files
    â†“
store_index.py (run manually)
    â†“
Pinecone (stays indexed)
    â†“
Used in every chat query
    â†“
Never changes (unless re-indexed)
```

### Evaluation Data
```
qa_dataset.json
    â†“
evaluate_with_visualization.py (run manually)
    â†“
Results written to CSV/JSON
    â†“
Not used in production chat
    â†“
Only for measuring performance
```

### MCP Local Data
```
User ingests dataset
    â†“
MCPDatasetServer loads into memory
    â†“
Used in every chat query
    â†“
Persists in session
    â†“
Cleared on app restart
```

### Web Data
```
Real-time lookup
    â†“
Exa API called on each medical question
    â†“
Results used in response
    â†“
Not cached
    â†“
Always current
```

---

## File Locations Summary

```
MedicalChatbot/
â”‚
â”œâ”€â”€ Data/                          â† TRAINING DATA
â”‚   â”œâ”€â”€ GALE_ENCYCLOPEDIA.pdf
â”‚   â”œâ”€â”€ medquad_dataset.json
â”‚   â”œâ”€â”€ medical_conditions.json
â”‚   â”œâ”€â”€ medical_diseases.csv
â”‚   â””â”€â”€ mimic_iv_reference.json
â”‚
â”œâ”€â”€ docs/evaluation/               â† EVALUATION DATA
â”‚   â”œâ”€â”€ qa_dataset.json
â”‚   â”œâ”€â”€ qa_dataset_expanded.json
â”‚   â””â”€â”€ evaluate_with_visualization.py
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mcp_server.py             â† MCP LOCAL DATA STORAGE
â”‚   â”œâ”€â”€ mcp_client.py             â† MCP INTERFACE
â”‚   â”œâ”€â”€ exa_web_search.py         â† WEB SEARCH
â”‚   â””â”€â”€ prompt.py                 â† SESSION MEMORY
â”‚
â”œâ”€â”€ store_index.py                â† INDEX TRAINING DATA
â”‚
â””â”€â”€ app.py                         â† CHAT APPLICATION
    â”œâ”€â”€ Uses Training data (Pinecone)
    â”œâ”€â”€ Uses MCP data (local)
    â”œâ”€â”€ Uses Web data (Exa API)
    â””â”€â”€ Uses session memory
```

---

## Data Size Estimates

| Data Type | Size | Chunks | Notes |
|-----------|------|--------|-------|
| GALE Encyclopedia | ~100MB | 2000+ | Largest source |
| medquad dataset | ~50MB | 1500+ | Q&A format |
| medical_conditions.json | ~500KB | 500+ | Structured |
| medical_diseases.csv | ~100KB | 100+ | Minimal |
| mimic_iv_reference.json | ~100KB | 200+ | Realistic data |
| **Training Total** | **~150MB** | **~4300+** | Pinecone indexed |
| Evaluation Q&A | ~100KB | N/A | Only 50-100 pairs |
| **Total** | **~150MB** | **~4300+** | For production |

---

## Key Points

âœ… **Training & Evaluation Separate** - No data leakage  
âœ… **Multiple Sources** - RAG + MCP + Web combined  
âœ… **Real-time Web** - Latest medical info available  
âœ… **Local Data Ingestion** - MCP for custom datasets  
âœ… **Fair Metrics** - Evaluation on unseen data  
âœ… **Complete Attribution** - Sources shown to user  

---

**Status**: âœ… Complete Data Integration  
**Sources**: 5 training files + evaluation set + MCP + Web API

