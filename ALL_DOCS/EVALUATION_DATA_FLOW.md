# Evaluation Script Data Flow - Complete Breakdown

## ğŸ“Š Which Data is Used?

The evaluation script `evaluate_with_visualization.py` uses **TWO completely different data sources**:

### 1. **Q&A Test Dataset** (Questions to test)
### 2. **Training Data from Pinecone** (Knowledge base to search)

---

## Data Source 1: Q&A Test Dataset

### File Loaded
```python
# Line 58-64
qa_dataset_path = Path(__file__).parent / 'qa_dataset_expanded.json'
if not qa_dataset_path.exists():
    qa_dataset_path = Path(__file__).parent / 'qa_dataset.json'

with open(qa_dataset_path, 'r') as f:
    qa_data = json.load(f)
```

### Location
```
ğŸ“ docs/evaluation/
â”œâ”€â”€ qa_dataset_expanded.json  â† Preferred (if exists)
â””â”€â”€ qa_dataset.json           â† Fallback
```

### What It Contains
```json
[
  {
    "question": "What are the common symptoms of diabetes mellitus?",
    "ground_truth": "Common symptoms include frequent urination, excessive thirst..."
  },
  {
    "question": "How is hypertension typically treated?",
    "ground_truth": "Treatment includes lifestyle modifications and medications..."
  }
  // ... 48-98 more Q&A pairs
]
```

### Usage in Script
```python
# Line 96-114 in prepare_evaluation_data()
qa_items = qa_data[:limit] if limit else qa_data  # Limit to 50 by default

for idx, item in enumerate(qa_items):
    question = item['question']           # â† Extract question
    ground_truth = item['ground_truth']   # â† Extract expected answer
    # ... process each pair
```

### Key Point
âœ… **This is the test set** - NOT from training data  
âœ… Used to evaluate RAG system performance  
âœ… 50-100 independent Q&A pairs  

---

## Data Source 2: Training Data from Pinecone

### Loaded From
```python
# Line 99-109 in prepare_evaluation_data()
embeddings = download_hugging_face_embeddings()
index_name = "medicalbot"
docsearch = PineconeVectorStore.from_existing_index(
    index_name=index_name,
    embedding=embeddings,
    namespace="default"
)
```

### Source Origin
The Pinecone index "medicalbot" contains **ALL** files from `Data/` directory:

```
ğŸ“ Data/ (indexed into Pinecone)
â”œâ”€â”€ The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf
â”œâ”€â”€ medquad_dataset.json
â”œâ”€â”€ medical_conditions.json
â”œâ”€â”€ medical_diseases.csv
â””â”€â”€ mimic_iv_reference.json
```

### How It's Indexed
```
store_index.py runs and:
1. Loads all Data/ files
2. Chunks them (~500-1000 tokens)
3. Creates embeddings (384-dim vectors)
4. Stores in Pinecone index "medicalbot"
```

### How It's Retrieved
```python
# Line 106-109
retriever = docsearch.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 8, "fetch_k": 20, "lambda_mult": 0.5}
)
# Returns: Top 8 documents using Max Marginal Relevance
```

### Key Point
âœ… **This is the training/knowledge base**  
âœ… Searched during evaluation to answer questions  
âœ… Same data used in production chat  
âœ… ~4,300+ chunks indexed  

---

## Complete Data Flow in Script

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          evaluate_with_visualization.py                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”‚
    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  qa_dataset.jsonâ”‚  â”‚ Pinecone Index   â”‚
â”‚  (Test Qs)      â”‚  â”‚ "medicalbot"     â”‚
â”‚  50-100 pairs   â”‚  â”‚ (Training data)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚
         â”‚ Line 114-139       â”‚ Line 101-109
         â”‚ for each Q&A:      â”‚ retriever.invoke()
         â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  For each Q&A:   â”‚
         â”‚                  â”‚
         â”‚ 1. Get question  â”‚
         â”‚    from qa_data  â”‚
         â”‚                  â”‚
         â”‚ 2. Search Pince. â”‚
         â”‚    with question â”‚
         â”‚                  â”‚
         â”‚ 3. Get docs from â”‚
         â”‚    training data â”‚
         â”‚                  â”‚
         â”‚ 4. Send to LLM   â”‚
         â”‚    (GPT-4o-mini) â”‚
         â”‚                  â”‚
         â”‚ 5. Get answer    â”‚
         â”‚                  â”‚
         â”‚ 6. Get contexts  â”‚
         â”‚    from retrievalâ”‚
         â”‚                  â”‚
         â”‚ 7. Prepare eval  â”‚
         â”‚    data with:    â”‚
         â”‚    - question    â”‚
         â”‚    - answer      â”‚
         â”‚    - contexts    â”‚
         â”‚    - ground_truthâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  RAGAS Eval      â”‚
         â”‚  (Line 142-158)  â”‚
         â”‚                  â”‚
         â”‚ Compute metrics: â”‚
         â”‚ â€¢ Faithfulness   â”‚
         â”‚ â€¢ Answer Relevancy
         â”‚ â€¢ Context Prec.  â”‚
         â”‚ â€¢ Context Recall â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Results Output  â”‚
         â”‚                  â”‚
         â”‚ â€¢ CSV results    â”‚
         â”‚ â€¢ JSON summary   â”‚
         â”‚ â€¢ PNG charts     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Step-by-Step Data Usage

### Step 1: Load Test Questions
```python
# Lines 58-64
qa_dataset_path = Path(__file__).parent / 'qa_dataset.json'
with open(qa_dataset_path, 'r') as f:
    qa_data = json.load(f)  # Load 50-100 Q&A pairs

print(f"Loaded {len(qa_data)} Q&A pairs")
```
**Data Used**: `qa_dataset.json` - evaluation questions

---

### Step 2: Setup RAG System
```python
# Lines 99-109 in prepare_evaluation_data()
embeddings = download_hugging_face_embeddings()

docsearch = PineconeVectorStore.from_existing_index(
    index_name="medicalbot",
    embedding=embeddings,
    namespace="default"
)

retriever = docsearch.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 8, "fetch_k": 20, "lambda_mult": 0.5}
)
```
**Data Used**: Pinecone index "medicalbot" (training data from Data/)

---

### Step 3: For Each Q&A Pair (50 iterations)
```python
# Lines 116-139
for idx, item in enumerate(qa_items):
    question = item['question']           # From qa_dataset.json
    ground_truth = item['ground_truth']   # From qa_dataset.json
    
    # Search training data
    retrieved_docs = retriever.invoke(question)  # Query Pinecone
    
    # Generate answer using LLM + retrieved docs
    response = rag_chain.invoke({"input": question})
    answer = response['answer']
    
    # Extract retrieved chunks
    contexts = [doc.page_content for doc in retrieved_docs]
    
    # Package for evaluation
    eval_data.append({
        'question': question,              # From qa_dataset.json
        'answer': answer,                  # Generated from LLM
        'contexts': contexts,              # Retrieved from Pinecone
        'reference': ground_truth          # From qa_dataset.json
    })
```

**Data Sources**:
- `question` & `ground_truth` â†’ from `qa_dataset.json`
- `retrieved_docs` & `contexts` â†’ from Pinecone (training data)
- `answer` â†’ generated by LLM using above

---

### Step 4: Run RAGAS Evaluation
```python
# Lines 142-158
dataset = Dataset.from_list(eval_data)  # 50 evaluation items

results = evaluate(dataset, metrics=[
    faithfulness,       # Does answer match contexts?
    answer_relevancy,   # Is answer relevant to question?
    context_precision,  # Are contexts relevant?
    context_recall      # Do contexts cover ground_truth?
])
```

**Data Used**: The 50 packed evaluation items from Step 3

---

### Step 5: Save Results
```python
# Lines 241-290 in save_results_summary()
results_df.to_csv('ragas_results_detailed.csv')

summary = {
    'metrics': {
        'faithfulness': 0.82,
        'answer_relevancy': 0.88,
        'context_precision': 0.79,
        'context_recall': 0.91
    }
}
```

**Output Files**:
- `ragas_results_detailed.csv` - All 50 Q&A scores
- `ragas_summary.json` - Aggregate statistics

---

## Data Diagram: What Flows Where?

```
qa_dataset.json (Test Set)
â”‚
â”œâ”€ Question 1: "What is diabetes?"
â”‚  â””â”€ Ground Truth: "Diabetes is a metabolic disease..."
â”‚
â”œâ”€ Question 2: "What is hypertension?"
â”‚  â””â”€ Ground Truth: "Hypertension is high blood pressure..."
â”‚
â””â”€ Question 50: "..."
   â””â”€ Ground Truth: "..."
        â”‚
        â”‚ Each question sent to:
        â–¼
    Pinecone Retriever
    (Training Data Index)
    â”‚
    â”œâ”€ From: GALE_ENCYCLOPEDIA.pdf
    â”‚  â””â”€ Retrieved: 2-3 chunks
    â”‚
    â”œâ”€ From: medquad_dataset.json
    â”‚  â””â”€ Retrieved: 2-3 chunks
    â”‚
    â”œâ”€ From: medical_conditions.json
    â”‚  â””â”€ Retrieved: 1-2 chunks
    â”‚
    â”œâ”€ From: medical_diseases.csv
    â”‚  â””â”€ Retrieved: 0-1 chunks
    â”‚
    â””â”€ From: mimic_iv_reference.json
       â””â”€ Retrieved: 0-1 chunks
           â”‚
           â”‚ Total: 8 chunks (k=8)
           â–¼
        LLM (GPT-4o-mini)
        â”‚
        â”œâ”€ Input: Question + 8 chunks
        â””â”€ Output: Answer
              â”‚
              â–¼
         RAGAS Evaluation
         â”‚
         â”œâ”€ Compare: Answer vs Ground Truth
         â”œâ”€ Compare: Contexts vs Question
         â””â”€ Calculate: 4 metrics
              â”‚
              â–¼
         Store Results
         â”‚
         â”œâ”€ ragas_results_detailed.csv
         â”œâ”€ ragas_summary.json
         â””â”€ PNG visualization charts
```

---

## Summary Table: Data Sources

| Component | Source | File | Purpose | Size |
|-----------|--------|------|---------|------|
| **Questions** | Evaluation | `qa_dataset.json` | What to test | 50-100 pairs |
| **Ground Truth** | Evaluation | `qa_dataset.json` | Expected answers | 50-100 pairs |
| **Training Docs** | Production | Pinecone "medicalbot" | RAG knowledge | ~4,300 chunks |
| **Embeddings** | HuggingFace | Model | Search vectors | 384-dim |
| **LLM** | OpenAI | GPT-4o-mini | Generate answers | API |
| **Metrics** | RAGAS | Framework | Evaluate quality | 4 metrics |

---

## Key Insights

### âœ… **Data Separation is Maintained**
- Test questions from `qa_dataset.json`
- Training data from Pinecone
- NO overlap = fair evaluation

### âœ… **RAG Workflow Simulated**
- Real chat experience replicated
- Same retriever settings as production
- Same LLM as production

### âœ… **Multiple Sources Evaluated**
- GALE Encyclopedia
- medquad dataset
- medical_conditions.json
- medical_diseases.csv
- mimic_iv_reference.json

All indexed in Pinecone and searched during evaluation

### âœ… **Comprehensive Metrics**
- Faithfulness (accuracy check)
- Answer Relevancy (relevance check)
- Context Precision (retrieval quality)
- Context Recall (coverage check)

---

## How to Run Evaluation

```bash
# Make sure training data is indexed first
python store_index.py

# Then run evaluation
python docs/evaluation/evaluate_with_visualization.py
```

**Output** (in `docs/evaluation/`):
- `ragas_results_detailed.csv` - Detailed scores
- `ragas_summary.json` - Aggregated metrics
- `metrics_distribution_post.png` - Visualization
- `pre_vs_post_comparison.png` - Improvement chart

---

**Status**: âœ… Data properly separated and used  
**Validity**: âœ… Fair evaluation methodology  
**Production Ready**: âœ… Yes

